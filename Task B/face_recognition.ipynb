{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b924513b",
   "metadata": {},
   "source": [
    "#### ðŸ“¦ Import Libraries\n",
    "\n",
    "Imports necessary Python modules such as PyTorch, torchvision, PIL, and other utilities for deep learning and image processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79470fb9-1365-4957-9d78-a0d2b13ea6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39f7bc",
   "metadata": {},
   "source": [
    "#### âš™ï¸ Device Setup & Reproducibility\n",
    "\n",
    "Here we configure the device (CPU/GPU) for training and set manual seeds to ensure reproducibility of results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5e04c5f-7469-485e-bb4a-23c89f9dc053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defe9c31",
   "metadata": {},
   "source": [
    "#### ðŸ§¾ Custom Dataset Loader\n",
    "\n",
    "This defines a custom PyTorch `Dataset` class (`FaceRecognitionDataset`) that:\n",
    "- Loads both clean and distorted face images.\n",
    "- Assigns class labels based on folder names.\n",
    "- Applies appropriate transformations during loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2d0440-18ce-45af-9875-570de75b9996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class FaceRecognitionDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.id_to_label = {}\n",
    "\n",
    "        person_folders = sorted(os.listdir(root_dir))\n",
    "        for idx, person_folder in enumerate(person_folders):\n",
    "            person_folder_path = os.path.join(root_dir, person_folder)\n",
    "            if not os.path.isdir(person_folder_path):\n",
    "                continue\n",
    "            self.id_to_label[idx] = person_folder\n",
    "\n",
    "            # Load clean images\n",
    "            for file in os.listdir(person_folder_path):\n",
    "                file_path = os.path.join(person_folder_path, file)\n",
    "                if os.path.isfile(file_path) and file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    if file == 'distortion':  # skip distortion folder here\n",
    "                        continue\n",
    "                    self.data.append(file_path)\n",
    "                    self.labels.append(idx)\n",
    "\n",
    "            # Load distorted images\n",
    "            distortion_folder = os.path.join(person_folder_path, 'distortion')\n",
    "            if os.path.isdir(distortion_folder):\n",
    "                for distorted_file in os.listdir(distortion_folder):\n",
    "                    distorted_path = os.path.join(distortion_folder, distorted_file)\n",
    "                    if os.path.isfile(distorted_path) and distorted_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.data.append(distorted_path)\n",
    "                        self.labels.append(idx)\n",
    "\n",
    "        self.data = np.array(self.data)\n",
    "        self.labels = np.array(self.labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return image, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98cd057",
   "metadata": {},
   "source": [
    "#### ðŸŽ¨ Image Transformations\n",
    "\n",
    "Image augmentations for training and normalization for validation/testing are applied using `torchvision.transforms`. These include resizing, horizontal flipping, normalization, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71e39272-b5db-418b-a892-618beff54a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d1cbf",
   "metadata": {},
   "source": [
    "#### ðŸ—ƒï¸ Dataset Loading\n",
    "\n",
    "We load the training and validation datasets using the custom class and prepare dataloaders with appropriate batch sizes and shuffling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261c024a-d23d-47e7-862a-6930527b7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes (train): 877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dir = r\"Task_B\\train\"\n",
    "val_dir = r\"Task_B\\val\"\n",
    "\n",
    "# Load train dataset\n",
    "train_dataset = FaceRecognitionDataset(train_dir, transform=train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_classes = len(train_dataset.id_to_label)\n",
    "print(f\"Number of classes (train): {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef30286",
   "metadata": {},
   "source": [
    "#### ðŸ§  Model Architecture: ResNet-50\n",
    "\n",
    "- Load a pretrained ResNet-50 model.\n",
    "- Freeze all layers except `layer4` and the final fully connected (`fc`) layer.\n",
    "- Replace the last `fc` layer with one suited for our number of classes (i.e., number of persons).\n",
    "## ðŸ§® Loss Function, Optimizer & LR Scheduler\n",
    "\n",
    "- Use CrossEntropyLoss with label smoothing for stable training.\n",
    "- AdamW optimizer is selected with weight decay.\n",
    "- Learning rate scheduling using `StepLR`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58c64ac2-1e53-418e-a81b-996ca16764a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model setup - pretrained ResNet50\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers except layer4 and fc\n",
    "for name, param in model.named_parameters():\n",
    "    if 'layer4' in name or 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Replace final layer\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfed921-2c98-4a49-addb-46e527ff9819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlap: set()\n"
     ]
    }
   ],
   "source": [
    "train_names = set(os.listdir(train_dir))\n",
    "val_names = set(os.listdir(val_dir))\n",
    "print(\"Overlap:\", train_names & val_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe35bba",
   "metadata": {},
   "source": [
    "#### ðŸš€ Training the Model\n",
    "\n",
    "This section trains the modified ResNet-50 model for a fixed number of epochs:\n",
    "- Tracks training loss and accuracy.\n",
    "- Updates the model weights using backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a7ce582-33b0-482c-8c4d-8de799bff657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Train Loss: 4.9673 Accuracy: 0.2665\n",
      "Epoch 2/10 Train Loss: 2.9802 Accuracy: 0.5847\n",
      "Epoch 3/10 Train Loss: 1.7365 Accuracy: 0.9127\n",
      "Epoch 4/10 Train Loss: 1.3007 Accuracy: 0.9872\n",
      "Epoch 5/10 Train Loss: 1.1948 Accuracy: 0.9957\n",
      "Epoch 6/10 Train Loss: 1.1430 Accuracy: 0.9988\n",
      "Epoch 7/10 Train Loss: 1.1226 Accuracy: 0.9996\n",
      "Epoch 8/10 Train Loss: 1.1150 Accuracy: 0.9995\n",
      "Epoch 9/10 Train Loss: 1.1092 Accuracy: 0.9998\n",
      "Epoch 10/10 Train Loss: 1.1041 Accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_acc = 0.0\n",
    "best_model_wts = None\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} Train Loss: {epoch_loss:.4f} Accuracy: {epoch_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67251b12",
   "metadata": {},
   "source": [
    "#### ðŸ’¾ Save the Trained Model\n",
    "\n",
    "Save the trained modelâ€™s state dictionary (`state_dict`) to disk for later inference or evaluation using `torch.save()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf6a7246-1fa7-4aec-ac8b-9513e609b228",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"face_recognition_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ed290f",
   "metadata": {},
   "source": [
    "#### ðŸ” Embedding Extraction Model\n",
    "\n",
    "A new class `FaceEmbeddingExtractor` is defined:\n",
    "- Uses ResNet-50 without the final classification layer.\n",
    "- Extracts 2048-dimensional feature vectors (embeddings) for each image.\n",
    "- Normalizes embeddings for cosine similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae15f3bc-28e0-473d-99a0-0a330a9f120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceEmbeddingExtractor(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        # Use all layers except final FC\n",
    "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # output shape (batch, 2048, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Normalize embeddings to unit norm\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df255ca",
   "metadata": {},
   "source": [
    "#### ðŸ§  Prepare Reference Embeddings (Validation Set)\n",
    "\n",
    "Extract clean image embeddings from the validation set.\n",
    "These embeddings are used later for cosine similarity comparison against distorted query images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bb172d2-f358-45c6-b7ac-b23cb0f773f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FaceEmbeddingExtractor(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = FaceEmbeddingExtractor(model).to(device)\n",
    "embedding_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cdc66-5025-4bd0-9a99-c469bf27f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings_from_folder(folder_path):\n",
    "    embeddings = []\n",
    "    images = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        if os.path.isfile(file_path) and file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            img = val_transform(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = embedding_model(img)\n",
    "            embeddings.append(emb.cpu())\n",
    "            images.append(file_path)\n",
    "    if embeddings:\n",
    "        embeddings = torch.cat(embeddings, dim=0)  \n",
    "    else:\n",
    "        embeddings = torch.empty(0)\n",
    "    return embeddings, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b0af268-b4aa-474b-a83e-3e959ed80a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare val reference embeddings (clean images only)\n",
    "val_persons = sorted(os.listdir(val_dir))\n",
    "val_refs = {}\n",
    "val_refs_labels = []\n",
    "val_refs_names = []\n",
    "\n",
    "for person_folder in val_persons:\n",
    "    person_folder_path = os.path.join(val_dir, person_folder)\n",
    "    if not os.path.isdir(person_folder_path):\n",
    "        continue\n",
    "\n",
    "    # Get clean images (exclude distortion folder)\n",
    "    clean_images_folder = person_folder_path\n",
    "    clean_embeddings, clean_image_paths = extract_embeddings_from_folder(clean_images_folder)\n",
    "    \n",
    "    # Remove embeddings of distortion folder images from clean images\n",
    "    clean_embeddings = []\n",
    "    clean_image_paths = []\n",
    "    for f in os.listdir(person_folder_path):\n",
    "        if f.lower() == 'distortion':\n",
    "            continue\n",
    "        file_path = os.path.join(person_folder_path, f)\n",
    "        if os.path.isfile(file_path) and f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            img = val_transform(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                emb = embedding_model(img)\n",
    "            clean_embeddings.append(emb.cpu())\n",
    "            clean_image_paths.append(file_path)\n",
    "    if clean_embeddings:\n",
    "        clean_embeddings = torch.cat(clean_embeddings, dim=0)\n",
    "        val_refs[person_folder] = clean_embeddings\n",
    "        val_refs_labels.append(person_folder)\n",
    "        val_refs_names.append(person_folder)\n",
    "    else:\n",
    "        val_refs[person_folder] = torch.empty(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5df572",
   "metadata": {},
   "source": [
    "#### ðŸ§ª Evaluate on Distorted Images (Validation Set)\n",
    "\n",
    "For each distorted image:\n",
    "- Extract its embedding.\n",
    "- Compare with all reference embeddings using cosine similarity.\n",
    "- Classify the image as belonging to the person with the highest similarity.\n",
    "- Compute accuracy on distorted validation images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ff81ea8-89cb-497b-87cc-3fbf7f8cec14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Verification Accuracy: 0.9990\n"
     ]
    }
   ],
   "source": [
    "# Now evaluate on distorted images\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for person_folder in val_persons:\n",
    "    person_folder_path = os.path.join(val_dir, person_folder)\n",
    "    distortion_folder = os.path.join(person_folder_path, 'distortion')\n",
    "    if not os.path.isdir(distortion_folder):\n",
    "        continue\n",
    "\n",
    "    distorted_images = [f for f in os.listdir(distortion_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    for distorted_image in distorted_images:\n",
    "        distorted_path = os.path.join(distortion_folder, distorted_image)\n",
    "        img = Image.open(distorted_path).convert(\"RGB\")\n",
    "        img = val_transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            dist_emb = embedding_model(img).cpu()\n",
    "\n",
    "        # Compare with all reference embeddings from val set\n",
    "        max_sim = -1\n",
    "        pred_person = None\n",
    "        for ref_person, ref_embs in val_refs.items():\n",
    "            if ref_embs.shape[0] == 0:\n",
    "                continue\n",
    "            # Compute cosine similarity with all reference embeddings for this person\n",
    "            sims = F.cosine_similarity(dist_emb, ref_embs)\n",
    "            max_s = sims.max().item()\n",
    "            if max_s > max_sim:\n",
    "                max_sim = max_s\n",
    "                pred_person = ref_person\n",
    "\n",
    "        total += 1\n",
    "        if pred_person == person_folder:\n",
    "            correct += 1\n",
    "\n",
    "accuracy = correct / total if total > 0 else 0\n",
    "print(f\"Validation Verification Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be954f0",
   "metadata": {},
   "source": [
    "#### ðŸ“Š Final Evaluation with Metrics (Validation Set and Training Set)\n",
    "\n",
    "Using `sklearn.metrics`, we evaluate:\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n",
    "\n",
    "This gives a deeper understanding of model performance, beyond just accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53c5d73a-6773-4199-a296-fabed8ce4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluation Metrics on Training Set:\n",
      "  - Accuracy : 1.0000\n",
      "  - Precision: 1.0000\n",
      "  - Recall   : 1.0000\n",
      "  - F1-Score : 1.0000\n",
      "\n",
      "ðŸ“Š Evaluation Metrics on Validation Set:\n",
      "  - Accuracy : 0.9990\n",
      "  - Precision: 0.9989\n",
      "  - Recall   : 0.9991\n",
      "  - F1-Score : 0.9989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load trained model and setup embedding model\n",
    "model.load_state_dict(torch.load(\"face_recognition_model.pt\"))\n",
    "embedding_model = FaceEmbeddingExtractor(model).to(device)\n",
    "embedding_model.eval()\n",
    "\n",
    "# --------- Utility to extract reference embeddings (clean images) ---------\n",
    "def extract_clean_references(persons, directory):\n",
    "    refs = {}\n",
    "    ref_names = []\n",
    "    for person_folder in persons:\n",
    "        folder_path = os.path.join(directory, person_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        clean_embeddings = []\n",
    "        for f in os.listdir(folder_path):\n",
    "            if f.lower() == 'distortion':\n",
    "                continue\n",
    "            file_path = os.path.join(folder_path, f)\n",
    "            if os.path.isfile(file_path) and f.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                img = Image.open(file_path).convert(\"RGB\")\n",
    "                img = val_transform(img).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    emb = embedding_model(img)\n",
    "                clean_embeddings.append(emb.cpu())\n",
    "        if clean_embeddings:\n",
    "            refs[person_folder] = torch.cat(clean_embeddings, dim=0)\n",
    "            ref_names.append(person_folder)\n",
    "        else:\n",
    "            refs[person_folder] = torch.empty(0)\n",
    "    return refs, ref_names\n",
    "\n",
    "# --------- Evaluation Function Using Cosine Similarity ---------\n",
    "def evaluate_embeddings(reference_embeddings, reference_labels, query_paths, true_labels, tag):\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "\n",
    "    for path, true_label in zip(query_paths, true_labels):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        img = val_transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = embedding_model(img).cpu()\n",
    "\n",
    "        max_sim = -1\n",
    "        pred_label = None\n",
    "        for label, emb_list in reference_embeddings.items():\n",
    "            if emb_list.shape[0] == 0:\n",
    "                continue\n",
    "            sims = F.cosine_similarity(emb, emb_list)\n",
    "            max_s = sims.max().item()\n",
    "            if max_s > max_sim:\n",
    "                max_sim = max_s\n",
    "                pred_label = label\n",
    "\n",
    "        all_preds.append(pred_label)\n",
    "        all_trues.append(true_label)\n",
    "\n",
    "    acc = accuracy_score(all_trues, all_preds)\n",
    "    prec = precision_score(all_trues, all_preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(all_trues, all_preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(all_trues, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    print(f\"\\nðŸ“Š Evaluation Metrics on {tag}:\")\n",
    "    print(f\"  - Accuracy : {acc:.4f}\")\n",
    "    print(f\"  - Precision: {prec:.4f}\")\n",
    "    print(f\"  - Recall   : {rec:.4f}\")\n",
    "    print(f\"  - F1-Score : {f1:.4f}\")\n",
    "\n",
    "# --------- Prepare reference embeddings ---------\n",
    "train_persons = sorted(os.listdir(train_dir))\n",
    "val_persons = sorted(os.listdir(val_dir))\n",
    "\n",
    "train_refs, train_ref_names = extract_clean_references(train_persons, train_dir)\n",
    "val_refs, val_ref_names = extract_clean_references(val_persons, val_dir)\n",
    "\n",
    "# --------- Prepare distorted query sets ---------\n",
    "def collect_distorted_queries(persons, root_dir):\n",
    "    query_paths = []\n",
    "    true_labels = []\n",
    "    for person_folder in persons:\n",
    "        distortion_folder = os.path.join(root_dir, person_folder, 'distortion')\n",
    "        if not os.path.isdir(distortion_folder):\n",
    "            continue\n",
    "        for img_file in os.listdir(distortion_folder):\n",
    "            if img_file.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "                query_paths.append(os.path.join(distortion_folder, img_file))\n",
    "                true_labels.append(person_folder)\n",
    "    return query_paths, true_labels\n",
    "\n",
    "train_query_paths, train_true_labels = collect_distorted_queries(train_persons, train_dir)\n",
    "val_query_paths, val_true_labels = collect_distorted_queries(val_persons, val_dir)\n",
    "\n",
    "# --------- Run Evaluation ---------\n",
    "evaluate_embeddings(train_refs, train_ref_names, train_query_paths, train_true_labels, tag=\"Training Set\")\n",
    "evaluate_embeddings(val_refs, val_ref_names, val_query_paths, val_true_labels, tag=\"Validation Set\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
